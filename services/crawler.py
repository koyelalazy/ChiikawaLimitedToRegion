import requests
import time
import concurrent.futures # ğŸ”¥ æ–°å¢ï¼šå¤šå·¥è™•ç†æ¨¡çµ„
from bs4 import BeautifulSoup
from services.location import REGION_KEYWORDS

BASE_URL = "https://www.jp-api.com/contents/NOD62/PGE{}/"
DOMAIN = "https://www.jp-api.com"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# ğŸ”¥ æå–å‡ºå–®é æŠ“å–çš„é‚è¼¯
def fetch_page(page):
    url = BASE_URL.format(page)
    items = []
    try:
        # print(f"æ­£åœ¨æŠ“å–ç¬¬ {page} é ...") # è¨»è§£æ‰é¿å… log å¤ªå¤š
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.encoding = resp.apparent_encoding
        if resp.status_code != 200: return []
        
        soup = BeautifulSoup(resp.content, "html.parser")
        links = soup.find_all("a", class_="lightbox")
        
        for link in links:
            name = link.get("title", "").strip()
            if not name:
                img = link.find("img")
                if img: name = img.get("alt", "").strip()
            if not name: continue

            src = link.get("href", "")
            img_url = DOMAIN + src if src.startswith("/") else src

            region = "å…¶ä»–"
            for r_key, keywords in REGION_KEYWORDS.items():
                if any(k in name for k in keywords):
                    region = r_key
                    break
            
            category = "other"
            if "ãƒ€ã‚¤ã‚«ãƒƒãƒˆã‚­ãƒ¼ãƒ›ãƒ«ãƒ€ãƒ¼" in name:
                category = "tag"
            elif "ã¬ã„ãã‚‹ã¿ã‚­ãƒ¼ãƒã‚§ãƒ¼ãƒ³" in name:
                category = "plush"
            elif "ã‚½ãƒƒã‚¯ã‚¹" in name or "é´ä¸‹" in name:
                category = "socks"

            items.append({
                "name": name,
                "image": img_url,
                "region": region,
                "category": category
            })
    except Exception as e:
        print(f"Page {page} error: {e}")
    
    return items

# ğŸ”¥ ä¸»ç¨‹å¼ï¼šæ”¹æˆä¸¦è¡Œè™•ç†
def run_crawler():
    print("ğŸš€ å•Ÿå‹•æ¥µé€Ÿçˆ¬èŸ² (å¤šåŸ·è¡Œç·’ç‰ˆ)...")
    start_time = time.time()
    all_items = []
    
    # è¨­å®šè¦æŠ“å¹¾é  (ä¾‹å¦‚ 1~10 é )
    pages = range(1, 11) 
    
    # åŒæ™‚é–‹ 5 å€‹åŸ·è¡Œç·’å»æŠ“
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        results = executor.map(fetch_page, pages)
        
    for res in results:
        all_items.extend(res)
        
    print(f"âœ… çˆ¬å–å®Œæˆï¼è€—æ™‚: {time.time() - start_time:.2f} ç§’ï¼Œå…± {len(all_items)} ç­†")
    return all_items